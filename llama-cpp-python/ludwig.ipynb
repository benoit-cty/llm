{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Llama fine-tuning with Ludwig\n",
    "\n",
    "\n",
    "Voir https://ludwig.ai/latest/ et https://colab.research.google.com/drive/1Ly01S--kUwkKQalE-75skalp-ftwl0fE?usp=sharing\n",
    "\n",
    "Il faut obtenir les droits sur Llama : https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n",
    "\n",
    "Puis les demander sur HF : https://huggingface.co/meta-llama/Llama-2-7b-hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ludwig[llm]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import Config, RepositoryEnv\n",
    "import os\n",
    "DOTENV_FILE = '../.env'\n",
    "env_config = Config(RepositoryEnv(DOTENV_FILE))\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = env_config('HUGGING_FACE_HUB_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_str = \"\"\"\n",
    "model_type: llm\n",
    "base_model: Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB\n",
    "#base_model: meta-llama/Llama-2-7b-hf\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "\n",
    "prompt:\n",
    "  template: |\n",
    "    ### Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    ### Input:\n",
    "    {input}\n",
    "\n",
    "    ### Response:\n",
    "\n",
    "input_features:\n",
    "  - name: prompt\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: output\n",
    "    type: text\n",
    "\n",
    "trainer:\n",
    "  type: finetune\n",
    "  learning_rate: 0.0001\n",
    "  batch_size: 1\n",
    "  gradient_accumulation_steps: 16\n",
    "  epochs: 1\n",
    "  learning_rate_scheduler:\n",
    "    warmup_fraction: 0.01\n",
    "\n",
    "preprocessing:\n",
    "  sample_ratio: 0.1\n",
    "\"\"\"\n",
    "\n",
    "config = yaml.safe_load(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a temporary directory at /tmp/tmpcy2fi60r\n",
      "Writing /tmp/tmpcy2fi60r/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 636/636 [00:00<00:00, 4.27MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤═══════════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ api_experiment                                                                            │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ run                                                                                       │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /media/data-nvme/dev/src/llm/llama-cpp-python/results/api_experiment_run_5                │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.8.2'                                                                                   │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/ipykernel_launcher.py ' │\n",
      "│                  │  '--ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 '                                  │\n",
      "│                  │  '--Session.signature_scheme=\"hmac-sha256\" '                                              │\n",
      "│                  │  '--Session.key=b\"d1e00ffd-f431-4697-b253-0a6444da13c8\" --shell=9002 '                    │\n",
      "│                  │  '--transport=\"tcp\" --iopub=9004 '                                                        │\n",
      "│                  │  '--f=/home/ben/.local/share/jupyter/runtime/kernel-v2-8564pYd7CaNamoV5.json')            │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ commit_hash      │ 'c49c4f4e7e5f'                                                                            │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                        │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ dataset          │ 'ludwig://alpaca'                                                                         │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ 'ludwig'                                                                                  │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '2.0.1+cu117'                                                                             │\n",
      "├──────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {'gpu_type': 'NVIDIA GeForce GTX 1080 Ti', 'gpus_per_node': 1, 'num_nodes': 1}            │\n",
      "╘══════════════════╧═══════════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'base_model': 'Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB',\n",
      "    'input_features': [{'name': 'prompt', 'type': 'text'}],\n",
      "    'ludwig_version': '0.8.2',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [{'name': 'output', 'type': 'text'}],\n",
      "    'preprocessing': {'sample_ratio': 0.1},\n",
      "    'prompt': {   'template': '### Instruction:\\n'\n",
      "                              '{instruction}\\n'\n",
      "                              '\\n'\n",
      "                              '### Input:\\n'\n",
      "                              '{input}\\n'\n",
      "                              '\\n'\n",
      "                              '### Response:\\n'},\n",
      "    'quantization': {'bits': 4},\n",
      "    'trainer': {   'batch_size': 1,\n",
      "                   'epochs': 1,\n",
      "                   'gradient_accumulation_steps': 16,\n",
      "                   'learning_rate': 0.0001,\n",
      "                   'learning_rate_scheduler': {'warmup_fraction': 0.01},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/media/data-nvme/dev/src/llm/llama-cpp-python/results/api_experiment_run_5/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sha256 digest provided for dataset alpaca, cannot verify.\n",
      "Contents:\n",
      "    alpaca_data.json: 2eddafc6b977608d778aaab8dfc7e50e547b3af9826dfb9e909d9fc362e4a419\n",
      "Found cached dataset and meta.json with the same filename of the dataset, but checksums don't match, if saving of processed input is not skipped they will be overridden\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 719/719 [00:00<00:00, 3.62MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.37MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 3.37MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'None': 204 (without start and stop symbols)\n",
      "Setting max length using dataset: 206 (including start and stop symbols)\n",
      "max sequence length is 206 for feature 'None'\n",
      "Loaded HuggingFace implementation of Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'output': 758 (without start and stop symbols)\n",
      "Setting max length using dataset: 760 (including start and stop symbols)\n",
      "max sequence length is 760 for feature 'output'\n",
      "Loaded HuggingFace implementation of Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /media/data-nvme/dev/src/llm/llama-cpp-python/ludwigalpaca.training.hdf5\n",
      "Writing preprocessed validation set cache to /media/data-nvme/dev/src/llm/llama-cpp-python/ludwigalpaca.validation.hdf5\n",
      "Writing preprocessed test set cache to /media/data-nvme/dev/src/llm/llama-cpp-python/ludwigalpaca.test.hdf5\n",
      "Writing train set metadata to /media/data-nvme/dev/src/llm/llama-cpp-python/ludwigalpaca.meta.json\n",
      "\n",
      "Dataset Statistics\n",
      "╒════════════╤═══════════════╤════════════════════╕\n",
      "│ Dataset    │   Size (Rows) │ Size (In Memory)   │\n",
      "╞════════════╪═══════════════╪════════════════════╡\n",
      "│ Training   │          3640 │ 853.25 Kb          │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Validation │           520 │ 122.00 Kb          │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Test       │          1040 │ 243.88 Kb          │\n",
      "╘════════════╧═══════════════╧════════════════════╛\n",
      "\n",
      "╒═══════╕\n",
      "│ MODEL │\n",
      "╘═══════╛\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)model.bin.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 57.5MB/s]\n",
      "Downloading (…)l-00001-of-00003.bin: 100%|██████████| 4.94G/4.94G [06:48<00:00, 12.1MB/s]\n",
      "Downloading (…)l-00002-of-00003.bin: 100%|██████████| 4.95G/4.95G [01:58<00:00, 41.6MB/s]\n",
      "Downloading (…)l-00003-of-00003.bin: 100%|██████████| 3.59G/3.59G [01:27<00:00, 41.1MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [10:17<00:00, 205.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.25s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 297/297 [00:00<00:00, 2.39MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of Trelis/Llama-2-7b-chat-hf-sharded-bf16-5GB tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n",
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "==================================================\n",
      "Tuning batch size...\n",
      "Tuning batch size...\n",
      "Exploring batch_size=1\n",
      "Throughput at batch_size=1: 0.40539 samples/s\n",
      "Exploring batch_size=2\n",
      "Throughput at batch_size=2: 0.33568 samples/s\n",
      "Throughput decrease at batch_size=2\n",
      "Selected batch_size=1\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 3640 step(s), approximately 1 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 18200 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training:   9%|▉         | 321/3640 [04:48<48:15,  1.15it/s, loss=0.194]   "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.91 GiB total capacity; 9.15 GiB already allocated; 107.81 MiB free; 9.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mludwig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m LudwigModel\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m LudwigModel(config\u001b[39m=\u001b[39mconfig, logging_level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mINFO)\n\u001b[0;32m----> 6\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mludwig://alpaca\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/ludwig/api.py:632\u001b[0m, in \u001b[0;36mLudwigModel.train\u001b[0;34m(self, dataset, training_set, validation_set, test_set, training_set_metadata, data_format, experiment_name, model_name, model_resume_path, skip_save_training_description, skip_save_training_statistics, skip_save_model, skip_save_progress, skip_save_log, skip_save_processed_input, output_directory, random_seed, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     callback\u001b[39m.\u001b[39mon_train_start(\n\u001b[1;32m    626\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m    627\u001b[0m         config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_obj\u001b[39m.\u001b[39mto_dict(),\n\u001b[1;32m    628\u001b[0m         config_fp\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_fp,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    631\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     train_stats \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    633\u001b[0m         training_set,\n\u001b[1;32m    634\u001b[0m         validation_set\u001b[39m=\u001b[39;49mvalidation_set,\n\u001b[1;32m    635\u001b[0m         test_set\u001b[39m=\u001b[39;49mtest_set,\n\u001b[1;32m    636\u001b[0m         save_path\u001b[39m=\u001b[39;49mmodel_dir,\n\u001b[1;32m    637\u001b[0m     )\n\u001b[1;32m    638\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, train_trainset_stats, train_valiset_stats, train_testset_stats) \u001b[39m=\u001b[39m train_stats\n\u001b[1;32m    640\u001b[0m     \u001b[39m# Calibrates output feature probabilities on validation set if calibration is enabled.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[39m# Must be done after training, and before final model parameters are saved.\u001b[39;00m\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/ludwig/trainers/trainer.py:812\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, training_set, validation_set, test_set, save_path, return_state_dict, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback(\u001b[39mlambda\u001b[39;00m c: c\u001b[39m.\u001b[39mon_epoch_start(\u001b[39mself\u001b[39m, progress_tracker, save_path))\n\u001b[1;32m    811\u001b[0m \u001b[39m# Trains over a full epoch of data.\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m should_break \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop(\n\u001b[1;32m    813\u001b[0m     batcher,\n\u001b[1;32m    814\u001b[0m     progress_tracker,\n\u001b[1;32m    815\u001b[0m     save_path,\n\u001b[1;32m    816\u001b[0m     train_summary_writer,\n\u001b[1;32m    817\u001b[0m     progress_bar,\n\u001b[1;32m    818\u001b[0m     training_set,\n\u001b[1;32m    819\u001b[0m     validation_set,\n\u001b[1;32m    820\u001b[0m     test_set,\n\u001b[1;32m    821\u001b[0m     start_time,\n\u001b[1;32m    822\u001b[0m     validation_summary_writer,\n\u001b[1;32m    823\u001b[0m     test_summary_writer,\n\u001b[1;32m    824\u001b[0m     model_hyperparameters_path,\n\u001b[1;32m    825\u001b[0m     output_features,\n\u001b[1;32m    826\u001b[0m     metrics_names,\n\u001b[1;32m    827\u001b[0m     checkpoint_manager,\n\u001b[1;32m    828\u001b[0m     final_steps_per_checkpoint,\n\u001b[1;32m    829\u001b[0m     early_stopping_steps,\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m \u001b[39m# ================ Post Training Epoch ================\u001b[39;00m\n\u001b[1;32m    833\u001b[0m progress_tracker\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/ludwig/trainers/trainer.py:951\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self, batcher, progress_tracker, save_path, train_summary_writer, progress_bar, training_set, validation_set, test_set, start_time, validation_summary_writer, test_summary_writer, model_hyperparameters_path, output_features, metrics_names, checkpoint_manager, final_steps_per_checkpoint, early_stopping_steps)\u001b[0m\n\u001b[1;32m    942\u001b[0m inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    943\u001b[0m     i_feat\u001b[39m.\u001b[39mfeature_name: torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(batch[i_feat\u001b[39m.\u001b[39mproc_column], copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    944\u001b[0m     \u001b[39mfor\u001b[39;00m i_feat \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minput_features\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    945\u001b[0m }\n\u001b[1;32m    946\u001b[0m targets \u001b[39m=\u001b[39m {\n\u001b[1;32m    947\u001b[0m     o_feat\u001b[39m.\u001b[39mfeature_name: torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(batch[o_feat\u001b[39m.\u001b[39mproc_column], copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    948\u001b[0m     \u001b[39mfor\u001b[39;00m o_feat \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39moutput_features\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    949\u001b[0m }\n\u001b[0;32m--> 951\u001b[0m loss, all_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(inputs, targets, should_step\u001b[39m=\u001b[39;49mshould_step)\n\u001b[1;32m    953\u001b[0m \u001b[39m# Update LR schduler here instead of train loop to avoid updating during batch size tuning, etc.\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/ludwig/trainers/trainer.py:270\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, inputs, targets, should_step)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_amp \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[1;32m    268\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed\u001b[39m.\u001b[39mprepare_model_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_model, should_step\u001b[39m=\u001b[39mshould_step):\n\u001b[1;32m    269\u001b[0m         \u001b[39m# Obtain model predictions and loss\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_model((inputs, targets))\n\u001b[1;32m    271\u001b[0m         loss, all_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain_loss(\n\u001b[1;32m    272\u001b[0m             targets, model_outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularization_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularization_lambda\n\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m         loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/ludwig/models/llm.py:354\u001b[0m, in \u001b[0;36mLLM.forward\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39m# Wrap with flash attention backend for faster generation\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msdp_kernel(enable_flash\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enable_math\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, enable_mem_efficient\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurr_device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m ) \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 354\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_inputs, attention_mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_masks)\u001b[39m.\u001b[39mget(LOGITS)\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_feature_type \u001b[39m!=\u001b[39m TEXT:\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Pass generated tokens through decoder after averaging the token probabilities\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# This is required for the classification head for the classifier decoder\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     model_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(model_outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/peft/peft_model.py:918\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    909\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    910\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    916\u001b[0m         )\n\u001b[0;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    919\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    920\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    921\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    922\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    923\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    924\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    925\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    926\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    929\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:94\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    807\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    808\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    809\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    810\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    811\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    812\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    815\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:421\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    419\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    420\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 421\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    422\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    424\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:216\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(down_proj)\n\u001b[1;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:221\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[1;32m    220\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 221\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul_4bit(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mt(), bias\u001b[39m=\u001b[39;49mbias, quant_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mquant_state)\n\u001b[1;32m    223\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    225\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[39mreturn\u001b[39;00m MatMul4Bit\u001b[39m.\u001b[39;49mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mempty(A\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m B_shape[:\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[39m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlinear(A, F\u001b[39m.\u001b[39;49mdequantize_4bit(B, state)\u001b[39m.\u001b[39mto(A\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[39m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m state\n",
      "File \u001b[0;32m/media/data-nvme/dev/src/llm/.venv/lib/python3.10/site-packages/bitsandbytes/functional.py:908\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m absmax\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mfloat32: absmax \u001b[39m=\u001b[39m absmax\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    907\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 908\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mempty(shape, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mA\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    910\u001b[0m n \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    913\u001b[0m device \u001b[39m=\u001b[39m pre_call(A\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.91 GiB total capacity; 9.15 GiB already allocated; 107.81 MiB free; 9.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "\n",
    "model = LudwigModel(config=config, logging_level=logging.INFO)\n",
    "results = model.train(dataset=\"ludwig://alpaca\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
